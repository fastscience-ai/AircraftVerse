{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "54c1f107-d6c6-4a78-b61f-04803b0edc06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch import nn, Tensor\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "import sys\n",
    "sys.path.insert(1, '../code/')\n",
    "import ssm\n",
    "# insert at 1, 0 is the script path (or '' in REPL)\n",
    "import util\n",
    "\n",
    "import math\n",
    "from typing import Tuple\n",
    "\n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer, TransformerDecoder, TransformerDecoderLayer\n",
    "from torch.utils.data import dataset\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "183ed61f-bc3f-499d-aad2-49009b4d3a04",
   "metadata": {},
   "source": [
    "### Set the path and specification that you want to predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "849f712d-ac8c-4bfc-be58-d334d4596838",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '../data/transformer_data'\n",
    "spec = 'airworthy' #'hover'#'interference'#'dist' #'mass' #'airworthy'\n",
    "max_mass = 35. #(kg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09bf7211-68d9-4d3e-bd6b-b8c15432df30",
   "metadata": {},
   "source": [
    "### Download the dataset and build the data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "e9f53380-d6c5-4f18-9925-7a5fc9ea1961",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "batch_size = 2 # 512\n",
    "batch_size_val = 2 # 512\n",
    "frac_train = 0.4\n",
    "frac_val = 0.4\n",
    "\n",
    "dataloader_tr, dataloader_val, dataloader_test, scale_1, scale_2 = ssm.prepare_sequence_data(data_path, spec, batch_size = batch_size ,batch_size_val = batch_size_val, frac_train = frac_train, frac_val = frac_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "97d66f86-f222-43e1-aa0b-2c3c455a6953",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['X', 'X_norm', 'y', 'airworthy', 'hover_time', 'max_speed', 'max_distance', 'interference_list', 'encoding_dict_keys', 'encoding_dict_values', 'norm_dict', 'path', 'folders'])\n"
     ]
    }
   ],
   "source": [
    "dic = torch.load(data_path)\n",
    "print(dic.keys())\n",
    "# dic['X'][-1]\n",
    "K = len(dic['encoding_dict_keys'])\n",
    "V = len(dic['encoding_dict_values'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7db6bbe0-ca0e-47a8-86e1-c0e1bb0c4bc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dic['X'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c1843c7c-e65c-40f8-a71c-9397648812c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([69, 43]), torch.Size([69, 577]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dic['X'][-6][:,:K].shape, dic['X'][-6][:,K:K+V].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8cb1bc7c-f587-4866-bc32-d6e54d99a7b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 69, 653]), torch.Size([3]), torch.Size([3, 69]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch, train_data=next(enumerate(dataloader_test))\n",
    "data, targets, mask = train_data\n",
    "data.size(), targets.size(), mask.size()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c4a3b181-6155-4228-860f-078956808f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_k=data[:,:,:K] #19\n",
    "X_v=data[:,:,K:K+V] #V (above code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "65545b96-1867-4d3f-9d3b-ba8051967f67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 69, 43]), torch.Size([37, 43]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_k.shape, dic['X'][-1][:,:K].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0b2374e2-a7af-4360-b149-b30b3df6c6e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 69, 577]), torch.Size([37, 577]), torch.Size([3, 69, 653]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_v.shape, dic['X'][-1][:,K:K+V].shape, data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d8f25069-72bc-40df-a799-08e625547cf4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([37, 653]), [1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dic['X'][-1].shape, dic['airworthy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a3cad948-a566-4f4d-911d-aca9302003b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([37])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dic['X'][-1][:,:K].argmax(1).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a9f65c59-7b95-400b-86c3-0b19aa2b56f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([19, 19, 10, 18, 27, 33, 41, 19,  2, 39, 24, 35, 21, 30, 12, 22, 36,  6,\n",
       "        15,  1,  3, 13, 16, 23, 37, 40, 34, 28, 29,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[-1][:,:K].argmax(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d1d6ac40-4da5-47e7-b7b3-63ec95107418",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43 577\n"
     ]
    }
   ],
   "source": [
    "print(K, V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aacb6c7a-b56d-4d35-950a-f04bc3ebb78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode(dic, tokens, values):\n",
    "    '''\n",
    "    Take an encoding dictionary {'token0':0, 'token1':1, ... } and list of tokens [0, 1, 4, ... ]\n",
    "    and return the corresponding strings of tokens ['token0', 'token1', 'token4', ...]\n",
    "    '''\n",
    "    \n",
    "    seq = []\n",
    "    dic_reverse = {value:key for key, value in dic.items()}\n",
    "    for n, tok in enumerate(tokens):\n",
    "        if dic_reverse[int(tok)] == 'Value':\n",
    "            seq.append(float(values[n]))\n",
    "        else:\n",
    "            seq.append(dic_reverse[int(tok)])\n",
    "    return seq\n",
    "\n",
    "    \n",
    "# decoded_design = []\n",
    "# for i in range(len(int(dic['X'][-1]):\n",
    "k = decode(dic['encoding_dict_keys'], dic['X'][-6][:,:K].argmax(1), None)\n",
    "v = decode(dic['encoding_dict_values'], dic['X'][-6][:,K:K+V].argmax(1), dic['X_norm'][-6][:,-1])\n",
    "    # decoded_design.append({k[0]:v[0]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "54a54cd8-6dff-447a-9e62-4be30a61c9b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'node_type': 'ConnectedHub4_1_2_1'},\n",
       " {'node_type': 'PropArm'},\n",
       " {'armLength': -0.9728665351867676},\n",
       " {'motorType': 't_motor_U7_V2.0KV490'},\n",
       " {'propType': 'apc_propellers_17x7E'},\n",
       " {'offset': -0.8203427195549011},\n",
       " {'angle': -0.34535443782806396},\n",
       " {'node_type': 'SidewaysBendWithTopSegment'},\n",
       " {'angle': -1.3278722763061523},\n",
       " {'armLength': 1.0017277002334595},\n",
       " {'node_type': 'PropArm'},\n",
       " {'armLength': -1.2462646961212158},\n",
       " {'motorType': 'kde_direct_KDE700XF_455_G3'},\n",
       " {'propType': 'apc_propellers_16x8'},\n",
       " {'offset': 1.362557053565979},\n",
       " {'angle': 1.7003532648086548},\n",
       " {'node_type': 'CrossSegment'},\n",
       " {'armLength': -1.3186391592025757},\n",
       " {'node_type': 'AngledPropArm'},\n",
       " {'armLength': 1.0142735242843628},\n",
       " {'motorType': 't_motor_U7_V2.0KV490'},\n",
       " {'propType': 'apc_propellers_16x6E'},\n",
       " {'node_type': 'AngledPropArm'},\n",
       " {'armLength': -1.4505165815353394},\n",
       " {'motorType': 'kde_direct_KDE4012XF_400'},\n",
       " {'propType': 'apc_propellers_17x7E'},\n",
       " {'node_type': 'CrossSegment'},\n",
       " {'armLength': 0.20251135528087616},\n",
       " {'node_type': 'AngledWingArm'},\n",
       " {'arm1Length': -1.1317706108093262},\n",
       " {'arm2Length': -0.4462025463581085},\n",
       " {'wingType': 'Wing_vert_hole'},\n",
       " {'nacaProfile': '4521'},\n",
       " {'span': -0.7071068286895752},\n",
       " {'chordInner': 0.7071068286895752},\n",
       " {'chordOuter': 0.7071067690849304},\n",
       " {'taperOffset': 1.0},\n",
       " {'aileronBias': -0.7071067690849304},\n",
       " {'flapBias': 0.70710688829422},\n",
       " {'load': -0.7071066498756409},\n",
       " {'tubeOffset': -0.7071067094802856},\n",
       " {'tubeRot': 0.0},\n",
       " {'servoType': 'Hitec_HS_82MG'},\n",
       " {'node_type': 'AngledPropArm'},\n",
       " {'armLength': -2.5833961963653564},\n",
       " {'motorType': 't_motor_AT4130KV450'},\n",
       " {'propType': 'apc_propellers_15x4W'},\n",
       " {'node_type': 'DualBatteryFuselageWithComponents'},\n",
       " {'batteryType': 'TurnigyGraphene5000mAh4S75C'},\n",
       " {'length': 0.08820804953575134},\n",
       " {'vertDiameter': -0.10397344082593918},\n",
       " {'horzDiameter': 0.9496257305145264},\n",
       " {'floorHeight': -0.10397353023290634},\n",
       " {'battery1X': 0.0},\n",
       " {'battery1Y': 1.1547006368637085},\n",
       " {'battery2X': 0.0},\n",
       " {'battery2Y': -1.1547002792358398},\n",
       " {'rpmX': 0.25819888710975647},\n",
       " {'rpmY': -0.949623703956604},\n",
       " {'autoPilotX': 0.25819888710975647},\n",
       " {'autoPilotY': 0.9496246576309204},\n",
       " {'currentX': 0.25819888710975647},\n",
       " {'currentY': -0.949623703956604},\n",
       " {'voltageX': 0.25819888710975647},\n",
       " {'voltageY': 0.949623703956604},\n",
       " {'gpsX': 0.25819888710975647},\n",
       " {'gpsY': -0.9496246576309204},\n",
       " {'varioX': 0.25819888710975647},\n",
       " {'varioY': 0.949623703956604}]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[{k[i]:v[i]} for i in range(len(k))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ea2b060b-9294-49ef-aff7-55e96c8f573f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_embedding(x):\n",
    "    ''' \n",
    "    x is one embedding sample [69(timestep), 653(embedding size)]\n",
    "    '''\n",
    "    def decode(dic, tokens, values):\n",
    "        '''\n",
    "        Take an encoding dictionary {'token0':0, 'token1':1, ... } and list of tokens [0, 1, 4, ... ]\n",
    "        and return the corresponding strings of tokens ['token0', 'token1', 'token4', ...]\n",
    "        '''\n",
    "    \n",
    "        seq = []\n",
    "        dic_reverse = {value:key for key, value in dic.items()}\n",
    "        for n, tok in enumerate(tokens):\n",
    "            if dic_reverse[int(tok)] == 'Value':\n",
    "                seq.append(float(values[n]))\n",
    "            else:\n",
    "                seq.append(dic_reverse[int(tok)])\n",
    "        return seq\n",
    "    data_path = '../data/transformer_data'\n",
    "    dic = torch.load(data_path)\n",
    "    K = len(dic['encoding_dict_keys'])\n",
    "    V = len(dic['encoding_dict_values'])\n",
    "    k = decode(dic['encoding_dict_keys'], x[:,:K].argmax(1), None)\n",
    "    v = decode(dic['encoding_dict_values'], x[:,K:K+V].argmax(1), dic['X_norm'][-6][:,-1])\n",
    "    end_idx = k.index('varioY') #end token\n",
    "    k = k[:end_idx+1]\n",
    "    v = v[:end_idx+1]\n",
    "    output = [{k[i]:v[i]} for i in range(len(k))]\n",
    "    return output\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "c397e1ce-40d7-4ec2-b0d7-7b7d3a03fb6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 69, 653])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch, test_data=next(enumerate(dataloader_test))\n",
    "data, targets, mask = test_data\n",
    "data.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "3a751861-5c53-4a8e-8fd5-2bca797b6d53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0000,  0.0000, -0.9729,  0.0000,  0.0000, -0.8203, -0.3454,  0.0000,\n",
       "        -1.3279,  1.0017,  0.0000, -1.2463,  0.0000,  0.0000,  1.3626,  1.7004,\n",
       "         0.0000, -1.3186,  0.0000,  1.0143,  0.0000,  0.0000,  0.0000, -1.4505,\n",
       "         0.0000,  0.0000,  0.0000,  0.2025,  0.0000, -1.1318, -0.4462,  0.0000,\n",
       "         0.0000, -0.7071,  0.7071,  0.7071,  1.0000, -0.7071,  0.7071, -0.7071,\n",
       "        -0.7071,  0.0000,  0.0000,  0.0000, -2.5834,  0.0000,  0.0000,  0.0000,\n",
       "         0.0000,  0.0882, -0.1040,  0.9496, -0.1040,  0.0000,  1.1547,  0.0000,\n",
       "        -1.1547,  0.2582, -0.9496,  0.2582,  0.9496,  0.2582, -0.9496,  0.2582,\n",
       "         0.9496,  0.2582, -0.9496,  0.2582,  0.9496])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dic['X_norm'][-6][:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "8bb222cb-7440-4909-9226-aef1e265ce6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000, -0.9729],\n",
       "         ...,\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000, -0.9496],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.2582],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.9496]],\n",
       "\n",
       "        [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000, -0.4256],\n",
       "         ...,\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "        [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.4963],\n",
       "         ...,\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "407717a6-cf62-4d12-9c09-1219a13aa59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = torch.clip(data, min=0, max = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "fa2fc3e7-af3f-4047-a1e9-a36ff2062a19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(69,\n",
       " 69,\n",
       " tensor([19, 19, 10, 18, 27, 33, 41, 19, 41, 10, 19, 10, 18, 27, 33, 41, 19, 10,\n",
       "         19, 10, 18, 27, 19, 10, 18, 27, 19, 10, 19, 14, 31,  4,  0,  9,  8, 32,\n",
       "         20, 26, 11, 42, 38,  5,  7, 19, 10, 18, 27, 19,  2, 39, 24, 35, 21, 30,\n",
       "         12, 22, 36,  6, 15,  1,  3, 13, 16, 23, 37, 40, 34, 28, 29]),\n",
       " tensor([221, 543, 576, 428, 361, 576, 576, 561, 576, 576, 543, 576,  35,  98,\n",
       "         576, 576, 246, 576, 513, 576, 428, 495, 513, 576,  15, 361, 246, 576,\n",
       "         328, 576, 576,  67, 353, 576, 576, 576, 576, 576, 576, 576, 576, 576,\n",
       "         354, 513, 576, 262, 484, 284, 305, 576, 576, 576, 576, 576, 576, 576,\n",
       "         576, 576, 576, 576, 576, 576, 576, 576, 576, 576, 576, 576, 576]),\n",
       " tensor([[ 19,  19,  10,  18, 647,  33,  41,  19,  41, 652,  19,  10, 621, 648,\n",
       "          652, 652,  19,  10,  19, 652,  18,  27,  19,  10,  18, 647,  19,  10,\n",
       "           19,  14,  31,   4,   0,   9,   8,  32,  20,  26,  11,  42,  38,   5,\n",
       "            7,  19,  10, 621, 650,  19, 638,  39,  24,  35,  21,  30, 652,  22,\n",
       "           36,   6,  15,   1,   3,  13,  16,  23,  37,  40,  34,  28,  29],\n",
       "         [ 19,  19,  10, 625, 647,  19, 652,  19,  10,  18,  27,  19,  10, 652,\n",
       "           19,  10,  18, 647,  19,  10,  18, 650,  33,  41,  19,  41,  14,  31,\n",
       "           19,  10,  18, 649,  33,  41,  19,  10,  18, 648,  19,   2, 652, 652,\n",
       "           35, 652,  25,  17,   6,  15,   1,   3,  13,  16,  23,  37,  40,  34,\n",
       "           28,  29,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "         [ 19,  19,  10, 621, 649,  33,  41,  19, 638,  39,  24, 652,  21,  30,\n",
       "           12,  22,  36,   6,  15,   1, 652,  13,  16,  23, 652,  40,  34,  28,\n",
       "          652,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "            0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "            0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0]]))"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data[0][:,:K].argmax(1)), len(data[0][:,K:K+V].argmax(1)), data[0][:,:K].argmax(1), data[0][:,K:K+V].argmax(1), data[:,:,:].argmax(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "b0f920c6-5182-4b07-a963-2014a6baac08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(69,\n",
       " 69,\n",
       " tensor([19, 19, 10, 18, 27, 33, 41, 19, 41, 10, 19, 10, 18, 27, 33, 41, 19, 10,\n",
       "         19, 10, 18, 27, 19, 10, 18, 27, 19, 10, 19, 14, 31,  4,  0,  9,  8, 32,\n",
       "         20, 26, 11, 42, 38,  5,  7, 19, 10, 18, 27, 19,  2, 39, 24, 35, 21, 30,\n",
       "         12, 22, 36,  6, 15,  1,  3, 13, 16, 23, 37, 40, 34, 28, 29]),\n",
       " tensor([221, 543, 576, 428, 361, 576, 576, 561, 576, 576, 543, 576,  35,  98,\n",
       "         576, 576, 246, 576, 513, 576, 428, 495, 513, 576,  15, 361, 246, 576,\n",
       "         328, 576, 576,  67, 353, 576, 576, 576, 576, 576, 576, 576, 576, 576,\n",
       "         354, 513, 576, 262, 484, 284, 305, 576, 576, 576, 576, 576, 576, 576,\n",
       "         576, 576, 576, 576, 576, 576, 576, 576, 576, 576, 576, 576, 576]))"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data2[0][:,:K].argmax(1)), len(data2[0][:,K:K+V].argmax(1)), data2[0][:,:K].argmax(1), data2[0][:,K:K+V].argmax(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "1754cb4c-7f0d-4a1d-84ea-7b98dd4ad255",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(1.8281),\n",
       " tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " torch.Size([2, 69, 653]),\n",
       " tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         1.]))"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.max(data[0]), data[0][:,1], data.size(), data[0][2,K:K+V]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7a3e1359-3f48-44ae-9e01-18291d359be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dic_reverse = {value:key for key, value in dic['encoding_dict_keys'].items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "66a38a6c-709c-45fb-98f4-bd8408fe5586",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'nacaProfile',\n",
       " 1: 'autoPilotX',\n",
       " 2: 'batteryType',\n",
       " 3: 'autoPilotY',\n",
       " 4: 'wingType',\n",
       " 5: 'tubeRot',\n",
       " 6: 'rpmX',\n",
       " 7: 'servoType',\n",
       " 8: 'chordInner',\n",
       " 9: 'span',\n",
       " 10: 'armLength',\n",
       " 11: 'flapBias',\n",
       " 12: 'battery1Y',\n",
       " 13: 'currentX',\n",
       " 14: 'arm1Length',\n",
       " 15: 'rpmY',\n",
       " 16: 'currentY',\n",
       " 17: 'batteryY',\n",
       " 18: 'motorType',\n",
       " 19: 'node_type',\n",
       " 20: 'taperOffset',\n",
       " 21: 'floorHeight',\n",
       " 22: 'battery2X',\n",
       " 23: 'voltageX',\n",
       " 24: 'vertDiameter',\n",
       " 25: 'batteryX',\n",
       " 26: 'aileronBias',\n",
       " 27: 'propType',\n",
       " 28: 'varioX',\n",
       " 29: 'varioY',\n",
       " 30: 'battery1X',\n",
       " 31: 'arm2Length',\n",
       " 32: 'chordOuter',\n",
       " 33: 'offset',\n",
       " 34: 'gpsY',\n",
       " 35: 'horzDiameter',\n",
       " 36: 'battery2Y',\n",
       " 37: 'voltageY',\n",
       " 38: 'tubeOffset',\n",
       " 39: 'length',\n",
       " 40: 'gpsX',\n",
       " 41: 'angle',\n",
       " 42: 'load'}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dic_reverse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cf8d7ed8-fd0d-4238-a1fe-21842d697437",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29,\n",
       " tensor([19, 19, 10, 18, 27, 33, 41, 19,  2, 39, 24, 35, 21, 30, 12, 22, 36,  6,\n",
       "         15,  1,  3, 13, 16, 23, 37, 40, 34, 28, 29,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(decode_embedding(data[2])), data[2][:,:K].argmax(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "05718478-3167-458d-950e-286234d0374f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'node_type': 'ConnectedHub6_Sym'},\n",
       " {'node_type': 'PropArm'},\n",
       " {'armLength': -0.9728665351867676},\n",
       " {'motorType': 't_motor_AT4130KV300'},\n",
       " {'propType': 'apc_propellers_14x14N'},\n",
       " {'offset': -0.8203427195549011},\n",
       " {'angle': -0.34535443782806396},\n",
       " {'node_type': 'DualBatteryFuselageWithComponents'},\n",
       " {'batteryType': 'TurnigyGraphene5000mAh6S75C'},\n",
       " {'length': 1.0017277002334595},\n",
       " {'vertDiameter': 0.0},\n",
       " {'horzDiameter': -1.2462646961212158},\n",
       " {'floorHeight': 0.0},\n",
       " {'battery1X': 0.0},\n",
       " {'battery1Y': 1.362557053565979},\n",
       " {'battery2X': 1.7003532648086548},\n",
       " {'battery2Y': 0.0},\n",
       " {'rpmX': -1.3186391592025757},\n",
       " {'rpmY': 0.0},\n",
       " {'autoPilotX': 1.0142735242843628},\n",
       " {'autoPilotY': 0.0},\n",
       " {'currentX': 0.0},\n",
       " {'currentY': 0.0},\n",
       " {'voltageX': -1.4505165815353394},\n",
       " {'voltageY': 0.0},\n",
       " {'gpsX': 0.0},\n",
       " {'gpsY': 0.0},\n",
       " {'varioX': 0.20251135528087616},\n",
       " {'varioY': 0.0}]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode_embedding(data[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f140695f-571d-43c7-835d-62eb83d757af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data:   6\n",
      "Validation Data: 6\n",
      "Test Data:       3\n"
     ]
    }
   ],
   "source": [
    "print(f'Training Data:   {dataloader_tr.dataset.y_train.shape[0]}')\n",
    "print(f'Validation Data: {dataloader_val.dataset.y_train.shape[0]}')\n",
    "print(f'Test Data:       {dataloader_test.dataset.y_train.shape[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "323d6ad0-03ea-4431-97a4-e2887a8eb95e",
   "metadata": {},
   "source": [
    "### Set up model from the seq_to_spec_model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6b9fd09e-7993-457c-827b-745f778a3d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "emsize = 200  # embedding dimension\n",
    "d_hid = 512  # dimension of the feedforward network model in nn.TransformerEncoder\n",
    "nlayers = 8  # number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
    "nhead = 20  # number of heads in nn.MultiheadAttention\n",
    "dropout = 0.2  # dropout probability\n",
    "D_out = 200\n",
    "D = dataloader_tr.dataset.x_train.shape[-1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f26f00c3-0d90-41bb-a817-8132a3a3fcd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "class TransformerEncoder(nn.Module):\n",
    "  def __init__(self, input_size, num_head, hidden_size, num_layers):\n",
    "\n",
    "    super(TransformerEncoder, self).__init__()\n",
    "\n",
    "    self.embd = nn.Embedding(word_count,input_size)\n",
    "    encoder_layer = nn.TransformerEncoderLayer(input_size, num_head, hidden_size)\n",
    "    self.transformer_enc = nn.TransformerEncoder(encoder_layer, num_layers)\n",
    "    self.linear1 = nn.Linear(input_size,270)\n",
    "\n",
    "def forward(self, x):\n",
    "    x = x.long()\n",
    "    emb = self.embd(x)\n",
    "    mem = self.transformer_enc(emb)\n",
    "    out = self.linear1(mem)\n",
    "    return out, mem\n",
    "\"\"\"\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, nhead: int, d_hid: int,\n",
    "                 nlayers: int, dropout: float = 0.01, D: int = 741,\n",
    "                 emsize: int = 200): #dropout was 0.5\n",
    "        super().__init__()\n",
    "        \n",
    "        self.D = D\n",
    "        self.D_out = emsize\n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
    "        encoder_layers = TransformerEncoderLayer(d_model, nhead, d_hid, dropout)\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n",
    "        self.encoder = nn.Linear(self.D, d_model)\n",
    "        self.d_model = d_model\n",
    "        self.decoder = nn.Linear(d_model, self.D_out)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self) -> None:\n",
    "        initrange = 0.1\n",
    "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
    "        self.decoder.bias.data.zero_()\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, src: Tensor, src_mask: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            src: Tensor, shape [seq_len, batch_size]\n",
    "            src_mask: Tensor, shape [seq_len, seq_len]\n",
    "\n",
    "        Returns:\n",
    "            output Tensor of shape [seq_len, batch_size, ntoken]\n",
    "        \"\"\"\n",
    "#         Need to permute from B x SL x D to SL x B x D\n",
    "        src = self.encoder(src.permute(1, 0, 2)) * math.sqrt(self.d_model)\n",
    "        src = self.pos_encoder(src)\n",
    "        mem = self.transformer_encoder(src, src_key_padding_mask=src_mask)\n",
    "        output = self.decoder(mem)\n",
    "        return output, mem\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor, shape [seq_len, batch_size, embedding_dim]\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x)\n",
    "\n",
    "def generate_square_subsequent_mask(sz: int) -> Tensor:\n",
    "    \"\"\"Generates an upper-triangular matrix of -inf, with zeros on diag.\"\"\"\n",
    "    return torch.triu(torch.ones(sz, sz) * float('-inf'), diagonal=1) \n",
    "    \n",
    "class TransformerDecoder(nn.Module):\n",
    "  def __init__(self, input_size, num_head, output_size, hidden_size, num_layers, dropout: float = 0.01):\n",
    "    super(TransformerDecoder, self).__init__()\n",
    "\n",
    "    decoder_layer = nn.TransformerDecoderLayer(input_size, num_head, hidden_size, dropout)\n",
    "    self.transformer_dec = nn.TransformerDecoder(decoder_layer, num_layers)\n",
    "    self.linear1 = nn.Linear(input_size, output_size)\n",
    "      # nn.linear(inpute_size, K+V+1)\n",
    "\n",
    "  def forward(self, x: Tensor, mem: Tensor, tgt_mask: Tensor = None)-> Tensor:\n",
    "    out = self.transformer_dec(x, mem) #, tgt_key_padding_mask=tgt_mask)\n",
    "    out = self.linear1(out)\n",
    "    return out\n",
    "\n",
    "class TransformerAutoencoder(nn.Module):\n",
    "    def __init__(self, d_model: int, nhead: int, d_hid: int,\n",
    "                 nlayers: int, dropout: float = 0.01, D: int = 741,\n",
    "                 emsize: int = 200): \n",
    "        super().__init__()\n",
    "        self.transformer_encoder = TransformerModel(d_model, nhead, d_hid, nlayers, dropout, D, emsize)\n",
    "        self.transformer_decoder = TransformerDecoder(d_model, nhead, 653, d_hid, nlayers, dropout)\n",
    "    def forward(self, src: Tensor, src_mask: Tensor) -> Tensor:\n",
    "        out, mem = self.transformer_encoder(src,src_mask)\n",
    "        print(out.size)\n",
    "        print(mem.size)\n",
    "        out  =self.transformer_decoder(out,mem, src_mask)\n",
    "        out = out.permute(1, 0, 2)\n",
    "        return out\n",
    "\n",
    "def loss(out):\n",
    "    l1 = crossentropy_loss(out[:K],X_k) #K=19 X_k: input up to K\n",
    "    l2 = crossentropy_loss(out[K:K+V],X_v)\n",
    "    if X_v == 'Value':\n",
    "        l3 = mse(out[-1], X_float)\n",
    "    return l1 + l2 + l3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "aea389c9-09d7-440a-b67b-5e2c1ff82e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "device='cpu'\n",
    "model = TransformerAutoencoder( emsize, nhead, d_hid, nlayers, dropout, D, D_out).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4bd3fdae-6c49-4226-9797-1e31617401cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch, train_data=next(enumerate(dataloader_tr))\n",
    "data, targets, mask = train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a417dfee-bb44-4f31-803e-85e85dabc7d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 69, 653]), torch.Size([2]), torch.Size([2, 69]))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.size(), targets.size(), mask.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "44869b67-59d8-4d79-934d-cb68c4d14ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_k: data[:,:,:K] #19\n",
    "X_v: data[:,:,K:K+V] #V (above code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9da8bd6a-dd8b-4a86-be7d-59e54774f210",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<built-in method size of Tensor object at 0x7fa317161350>\n",
      "<built-in method size of Tensor object at 0x7fa3170fa2a0>\n"
     ]
    }
   ],
   "source": [
    "output= model(data.to(device), mask.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b765f24c-34dd-4269-b358-c51c80ab9a49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 69, 653])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c288f0-5045-4e07-bb73-72c90e296a79",
   "metadata": {},
   "source": [
    "### Set up the training routine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "09b569a0-2aa3-4ab7-8a87-d5083bbd1dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import time\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "    \n",
    "lr = .1  # learning rate\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "# scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 100.0, gamma=0.95)\n",
    "\n",
    "def train(model: nn.Module) -> None:\n",
    "    model.train()  # turn on train mode\n",
    "    total_loss = 0.\n",
    "    log_interval = 200\n",
    "    start_time = time.time()\n",
    "\n",
    "    num_batches = 0 #bptt\n",
    "    for batch, train_data in enumerate(dataloader_tr):\n",
    "        data, targets, mask = train_data\n",
    "        output = model(data.to(device), mask.to(device))\n",
    "        loss = criterion(output, data)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        num_batches += 1\n",
    "\n",
    "    return total_loss/num_batches\n",
    "\n",
    "def evaluate(model: nn.Module) -> float:\n",
    "    model.eval()  # turn on evaluation mode\n",
    "    total_loss = 0.\n",
    "    num_batches = 0\n",
    "    with torch.no_grad():\n",
    "        for batch, val_data in enumerate(dataloader_val):\n",
    "            data, targets, mask = val_data\n",
    "            output = model(data.to(device), mask.to(device))\n",
    "            loss = criterion(output, data)\n",
    "            num_batches += 1\n",
    "            total_loss += loss.item()\n",
    "    return total_loss/num_batches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b138921-061b-4c95-8b74-4969def303f6",
   "metadata": {},
   "source": [
    "### Train the model and save the best according to the validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7d3a967a-87af-4850-8625-adf0c01d852d",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<built-in method size of Tensor object at 0x7fa317166c00>\n",
      "<built-in method size of Tensor object at 0x7fa317494310>\n",
      "<built-in method size of Tensor object at 0x7fa317166e30>\n",
      "<built-in method size of Tensor object at 0x7fa3174990d0>\n",
      "<built-in method size of Tensor object at 0x7fa316fbef20>\n",
      "<built-in method size of Tensor object at 0x7fa3174a9170>\n",
      "<built-in method size of Tensor object at 0x7fa3170c5440>\n",
      "<built-in method size of Tensor object at 0x7fa3171670b0>\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | time:  5.68s | loss 0.2991 | \n",
      "-----------------------------------------------------------------------------------------\n",
      "<built-in method size of Tensor object at 0x7fa3168c05e0>\n",
      "<built-in method size of Tensor object at 0x7fa2ea3c1710>\n",
      "<built-in method size of Tensor object at 0x7fa317167dd0>\n",
      "<built-in method size of Tensor object at 0x7fa2ea3c1a80>\n",
      "<built-in method size of Tensor object at 0x7fa3170f4950>\n",
      "<built-in method size of Tensor object at 0x7fa2ea3c1760>\n",
      "<built-in method size of Tensor object at 0x7fa317495d50>\n",
      "<built-in method size of Tensor object at 0x7fa2ea3c0d10>\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | time:  4.97s | loss 0.2270 | \n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "best_loss = float('inf')\n",
    "epochs = 2 #300\n",
    "best_model = None\n",
    "\n",
    "loss_list = []\n",
    "val_loss_list = []\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    epoch_start_time = time.time()\n",
    "    loss = train(model)\n",
    "    val_loss = evaluate(model)\n",
    "    elapsed = time.time() - epoch_start_time\n",
    "    print('-' * 89)\n",
    "    print(f'| end of epoch {epoch:3d} | time: {elapsed:5.2f}s | '\n",
    "          f'loss {loss:5.4f} | ')\n",
    "          #f'val loss {val_loss:5.4f}' )\n",
    "    print('-' * 89)\n",
    "\n",
    "    if val_loss < best_loss:\n",
    "        best_loss = val_loss\n",
    "        best_model = copy.deepcopy(model)\n",
    "    loss_list.append(loss)\n",
    "    val_loss_list.append(val_loss)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "00c6f19f-3911-4a9d-814d-f0116341b86d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Save and load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3d35a183-5f5b-4039-b06f-41526f41dd56",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, \"./temp.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5027ce50-4c6b-4829-aad1-66a746c8f538",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "825426da-6621-4d76-97a6-e2fc0d4e2727",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TransformerAutoencoder( emsize, nhead, d_hid, nlayers, dropout, D, D_out).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3304143c-8960-433a-957e-2c15f340774f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load(\"./temp.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6f11c297-bc53-4051-b548-42cfcb14bcce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerAutoencoder(\n",
       "  (transformer_encoder): TransformerModel(\n",
       "    (pos_encoder): PositionalEncoding(\n",
       "      (dropout): Dropout(p=0.2, inplace=False)\n",
       "    )\n",
       "    (transformer_encoder): TransformerEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-7): 8 x TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=200, out_features=200, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=200, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.2, inplace=False)\n",
       "          (linear2): Linear(in_features=512, out_features=200, bias=True)\n",
       "          (norm1): LayerNorm((200,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((200,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.2, inplace=False)\n",
       "          (dropout2): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (encoder): Linear(in_features=653, out_features=200, bias=True)\n",
       "    (decoder): Linear(in_features=200, out_features=200, bias=True)\n",
       "  )\n",
       "  (transformer_decoder): TransformerDecoder(\n",
       "    (transformer_dec): TransformerDecoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-7): 8 x TransformerDecoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=200, out_features=200, bias=True)\n",
       "          )\n",
       "          (multihead_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=200, out_features=200, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=200, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.2, inplace=False)\n",
       "          (linear2): Linear(in_features=512, out_features=200, bias=True)\n",
       "          (norm1): LayerNorm((200,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((200,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm3): LayerNorm((200,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.2, inplace=False)\n",
       "          (dropout2): Dropout(p=0.2, inplace=False)\n",
       "          (dropout3): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (linear1): Linear(in_features=200, out_features=653, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "cb4e91bd-5f28-49c2-b800-bd983f76e956",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 69, 653]) torch.Size([2, 69])\n",
      "TransformerModel(\n",
      "  (pos_encoder): PositionalEncoding(\n",
      "    (dropout): Dropout(p=0.2, inplace=False)\n",
      "  )\n",
      "  (transformer_encoder): TransformerEncoder(\n",
      "    (layers): ModuleList(\n",
      "      (0-7): 8 x TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=200, out_features=200, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=200, out_features=512, bias=True)\n",
      "        (dropout): Dropout(p=0.2, inplace=False)\n",
      "        (linear2): Linear(in_features=512, out_features=200, bias=True)\n",
      "        (norm1): LayerNorm((200,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((200,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.2, inplace=False)\n",
      "        (dropout2): Dropout(p=0.2, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (encoder): Linear(in_features=653, out_features=200, bias=True)\n",
      "  (decoder): Linear(in_features=200, out_features=200, bias=True)\n",
      ")\n",
      "torch.Size([69, 2, 200]) torch.Size([69, 2, 200])\n",
      "torch.Size([69, 2, 200])\n",
      "torch.Size([69, 2, 200])\n",
      "torch.Size([2, 69, 200])\n",
      "torch.Size([2, 69, 653])\n"
     ]
    }
   ],
   "source": [
    "batch, data = next(enumerate(dataloader_tr))\n",
    "data, targets, mask = data\n",
    "print(data.size(), mask.size())\n",
    "print( model.transformer_encoder)\n",
    "embedding, mem = model.transformer_encoder(data, mask)\n",
    "print(embedding.size(), mem.size())\n",
    "embedding_copy = model.transformer_encoder.decoder(mem)\n",
    "print(embedding_copy.size())\n",
    "#mem=mem.permute(1,0,2)\n",
    "print(mem.size())\n",
    "mem2 = mem.permute(1,0,2)\n",
    "print(mem2.size())\n",
    "out  =model.transformer_decoder(embedding,mem,mask)\n",
    "out = out.permute(1, 0, 2)\n",
    "print(out.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38107cdd-92f6-4de1-8f60-b6f64b8621dc",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Plot the training and validation loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb6895eb-c325-49a9-9c31-859adbf85340",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.plot(loss_list)\n",
    "plt.plot(val_loss_list)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "3518fcf1-c5a3-494c-8a02-3cb4f371b985",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 69, 653])\n",
      "<built-in method size of Tensor object at 0x7f9b060ac4f0>\n",
      "<built-in method size of Tensor object at 0x7f9b1139ffb0>\n",
      "MSE btw input and output of autoencoder 0.004527577199041843\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for x,y,m in dataloader_test:\n",
    "    with torch.no_grad():\n",
    "        print(x.shape)\n",
    "        best_model.eval().to(device)\n",
    "        output = best_model(x.to(device), m.to(device)).cpu()\n",
    "        print(\"MSE btw input and output of autoencoder\",criterion(x, output).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63785c8b-0716-44e4-8a84-238ef38caaca",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Convert Output of Autoencoder back to original sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "052b57e2-6e28-43d3-87ea-23d8dfe1c20b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
